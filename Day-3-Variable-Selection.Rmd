---
title: "Day 3 Variable Selection"
author: "Rachel Mercaldo"
date: "2025-07-21"
output: html_document
---

Welcome to the second (third? It never ends...) part of the day.

We are going to dive into a very important topic, that of variable selection.

While we have a very small data set for the workshop, you will have to work with a very large dataset when you analyze your data. Remember how we started with 97 variables? There will be even more! 

Here, we will practice using some tools that will help us decide which of those variables are important for us to keep for our final models. 

We will need some new packages:
```{r setup, message=FALSE}
# 
# install.packages("glmnet")
# install.packages("MASS")
# install.packages("randomForest")
library(data.table)
library(tidyverse)
library(zoo)
library(glmnet)
library(MASS)
library(randomForest)

```


You can continue using the data we already have in our working directory, or you can reload the data that we saved in the last part this morning's code:

```{r}
# Load data
data <- read.csv("final_data.csv")

str(data)
```


IF you loaded the data again, take note: having data in .csv files is really convenient, but it does remove a lot of formatting. Our month_year variable is probably a character again, and we need to feed it through the as.yearmon() function again:

```{r}
data <- data %>%
  mutate(month_year = as.yearmon(month_year, format = "%m/%Y")) 

head(data)

data <- data[complete.cases(data),]

head(data)
```

Now we need to tell R which variables are up for selection. The contenders. There are a couple ways of doing this. 

1. List the variables by name, within c()
2. Select a bunch of variables using indexing: data[,c(2:5)], etc.

Below I list them by name. Take a look at "data" and see if you can get the same result using indexing:

```{r}
# vars <- c("temperature", "precipitation", "pm2p5", "humidity", "wind_speed", "lag_temperature") #etc
# 
# #or

vars <- colnames(data[,4:13]) #what does colnames() do?

vars

```

Let's use a new select() option! You can combine it with another function, like any_of() or all_of(). 

Below, we will use select() to select our *predictor* data, or, the X data. Our case data, our dependent variable, will be saved to the object Y.

```{r}
# Prepare predictor matrix and response vector

vars 

X <- data %>% 
  dplyr::select(all_of(vars)) 


Y <- data$case_rate_100k
```

We are going to explore three different variable selection methods, stepwise variable selection, a LASSO model, and a Random Forest model.


1. Stepwise Variable Selection
```{r}
# Combine response and predictors
data_model <- data.frame(case_rate = Y, X)

# First, create a full model (all variables in data)
full_model <- lm(case_rate ~ ., data = data_model)
summary(full_model)

# Stepwise selection
step_model <- stepAIC(full_model, direction = "both", trace = FALSE)
summary(step_model)


test_model <- lm(case_rate_100k ~ wind_speed + lag_wind_speed + fips, data = data)
summary(test_model)
```

Interpretation: The selected variables are those retained in step_model.

2. LASSO Regression
Method: Using glmnet for penalized regression with variable selection

```{r}
# Prepare matrix for glmnet
X_mat <- as.matrix(X) #LASSO prefers a matrix format

# Fit LASSO (alpha=1)
set.seed(4) #we need to set a seed so we get the same result every time.

lasso_model <- cv.glmnet(X_mat, Y, alpha = 1) #the CV stands for cross-validation.

# Best lambda
best_lambda <- lasso_model$lambda.min 

# Coefficients
coef(lasso_model, s = best_lambda)

# Plot CV curve
plot(lasso_model)
```

Interpretation:
Variables with non-zero coefficients at lambda.min are selected.

In our case (with the variables we are using for the workshop, in this one climate zone), LASSO didn't pull out any variables except the intercept.

```{r}
lasso_variables <- c("temperature", "precipitation","pm2p5","wind_speed","humidity", "lag_temperature","lag_humidity","lag_wind_speed")
```


3. Random Forest Variable Importance
Method: Using randomForest to fit and extract variable importance

```{r}
# Fit Random Forest
set.seed(123)
rf_model <- randomForest(case_rate ~ ., data = data.frame(case_rate = Y, X), importance=TRUE)

# Variable importance
importance(rf_model)

# Plot importance
varImpPlot(rf_model)
```

Interpretation:
Variables with higher importance scores contribute more to the model's predictions.


Now we know what variables are important. We can choose a specific method or set some criteria (such as, final variables must have been selected as important in 2 of the 3 methods) 



## Part 2

Let's look at final models.


Stepwise:

The good news: a stepwise model can serve as a final model! Score! 

```{r}

summary(step_model)

```


LASSO (Least Absolute Shrinkage and Selection Operator)

Our LASSO model didn't find any non-zero variables except the intercept, but we can print the intercept's coefficient:

```{r}
lasso_data <- data[,c(lasso_variables,"case_rate_100k")]

head(lasso_data)

lasso_model <- glm(case_rate_100k ~ ., data = lasso_data)
summary(lasso_model)

```


Random Forest

```{r}
# Select the top important variables (based on %incMSE)
mse_vars_rf <- names(sort(importance(rf_model)[, 1], decreasing = TRUE)[1:3])
mse_vars_rf

# Select the top important variables (based on IncNodePurity)
purity_vars_rf <- names(sort(importance(rf_model)[, 2], decreasing = TRUE)[1:3])
purity_vars_rf
```


Random Forest Model with GLM (generalized linear model)

Based on top 3 according to MSE:

```{r}
# Formula for GLM model
rf_glm_formula <- as.formula(paste("case_rate_100k ~", paste(mse_vars_rf, collapse = " + ")))

rf_glm_model <- glm(rf_glm_formula, data = data)

# Evaluate model performance
summary(rf_glm_model)
```

Based on top 3 according to node purity:

```{r}
# Formula for GLM model
rf_glm_formula2 <- as.formula(paste("case_rate_100k ~", paste(purity_vars_rf, collapse = " + ")))

rf_glm_model2 <- glm(rf_glm_formula2, data = data)

# Evaluate model performance
summary(rf_glm_model2)
```